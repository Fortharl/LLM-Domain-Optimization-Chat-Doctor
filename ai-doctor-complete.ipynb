{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd53092",
   "metadata": {},
   "source": [
    "# <font color=\"#2E86C1\">Отчет об оценке качества медицинских консультаций</font>\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=\"#1F618D\"><u>1. Основные инсайты исследования</u></font>\n",
    "\n",
    "На основе анализа метрик и сгенерированных ответов выделены следующие ключевые закономерности:\n",
    "\n",
    "* **RAG — гарант безопасности:** Применение RAG (Retrieval-Augmented Generation) является критически важным для медицинской безопасности. Оценка выявления <font color=\"#C0392B\">**Red Flags**</font> возрастает более чем в 2 раза (с **2.36** до **4.49**).\n",
    "* **ИИ против Человека:** В данной выборке модели ИИ (особенно Base + RAG) превзошли «эталонные» ответы врачей по клинической точности и полноте плана действий. Это объясняется тем, что реальные ответы врачей часто были слишком краткими и игнорировали предупреждения о рисках.\n",
    "* **Скорость против Качества:** Тонкая настройка (**QLoRA**) значительно ускоряет генерацию (в 3-5 раз), но ведет к деградации способности модели распознавать критические ситуации. Это объясняется перениманием поведения из обучающей выборки на ответах врачей.\n",
    "* **Стиль и лаконичность:** Модели QLoRA лучше имитируют стиль «быстрого чата», в то время как базовые модели склонны к избыточности, если не ограничен системный промпт.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=\"#1F618D\"><u>2. Сравнительная таблица метрик</u></font>\n",
    "\n",
    "В таблице представлены усредненные значения для ключевых конфигураций.\n",
    "| Модель* | Latency (s) | Prompt | Generated | Total | Clinical correctness & factual accuracy | Appropriate triage & urgency guidance | Safety | Differential diagnosis quality | Actionability | <font color=\"#C0392B\">Red Flags</font> | Communication quality |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| **Doctor Answer** | — | — | — | — | 3.41 | 3.24 | 3.47 | 2.64 | 3.22 | 1.68 | 3.30 |\n",
    "| **Base Qwen3B** | 7.79 | 129.3 | 327.2 | 456.5 | 4.01 | 4.17 | 4.37 | 3.47 | 3.99 | 2.36 | 4.54 |\n",
    "| **Base + SysPrompt** | 8.01 | 448.3 | 295.2 | 743.6 | 3.85 | 3.91 | 4.15 | 3.13 | 3.55 | 2.08 | 4.53 |\n",
    "| <font color=\"#21618C\">**Base + RAG**</font> | <font color=\"#7B241C\">12.01</font> | 341.6 | 478.3 | 819.8 | **3.99** | **4.53** | **4.70** | **3.54** | **4.60** | **4.49** | **4.85** |\n",
    "| **Base + Sys+RAG** | 7.90 | 549.6 | 271.1 | 820.6 | 3.78 | 3.92 | 4.09 | 2.95 | 3.46 | 2.11 | 4.47 |\n",
    "| <font color=\"#1D8348\">**Cleaned QLoRA**</font> | **2.38** * | 129.3 | **85.5** | **214.9** | 3.52 | 3.23 | 3.80 | 2.53 | 2.90 | <font color=\"#7B241C\">1.26</font> | 3.43 |\n",
    "| **QLoRA + SysPrompt** | 4.44 | 448.3 | 154.9 | 603.2 | 3.70 | 3.55 | 3.97 | 2.87 | 3.33 | 1.28 | 3.96 |\n",
    "| **QLoRA + RAG** | 5.36 | 685.6 | 176.3 | 861.9 | 3.62 | 3.77 | 4.07 | 2.86 | 3.49 | 2.24 | 3.78 |\n",
    "| **QLoRA + Sys+RAG** | 4.31 | 549.6 | 144.0 | 693.6 | 3.44 | 3.41 | 3.77 | 2.57 | 2.90 | 1.33 | 3.62 |\n",
    "\n",
    "\\*Все модели в INT8\n",
    "**Низкая Latency за счет \"выученной\" через адаптер краткости.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=\"#1F618D\"><u>3. Сильные и слабые стороны подходов</u></font>\n",
    "\n",
    "#### **Подход: Base Model + RAG**\n",
    "\n",
    "* <font color=\"#145A32\">**Сильные стороны:**</font> Исключительная детализация, четкое структурирование (симптомы, рекомендации, предупреждения), высокая надежность в сложных случаях.\n",
    "* <font color=\"#7B241C\">**Слабые стороны:**</font> Самая высокая задержка ответа (до 12 секунд) и большой расход токенов.\n",
    "\n",
    "#### **Подход: Cleaned DATA QLoRA**\n",
    "\n",
    "* <font color=\"#145A32\">**Сильные стороны:**</font> Мгновенная реакция (2.4 сек), естественный «человеческий» тон, отсутствие лишней «воды».\n",
    "* <font color=\"#7B241C\">**Слабые стороны:**</font> Игнорирование опасных симптомов (низкий Red Flags), риск пропуска серьезных диагнозов в угоду краткости.\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=\"#1F618D\"><u>4. Анализ гипотез: когда подход эффективен?</u></font>\n",
    "\n",
    "1. **Сценарий «Тревожный симптом» (например, обморок, высокая температура у младенца):**\n",
    "* **Хорошо:** <font color=\"#27AE60\">Base + RAG</font>. Она выдает список критических признаков, при которых нужно немедленно вызвать скорую помощь.\n",
    "* **Плохо:** <font color=\"#C0392B\">QLoRA</font>. Может ограничиться общим советом «записаться к врачу», что опасно для жизни пациента.\n",
    "\n",
    "\n",
    "2. **Сценарий «Информационный запрос» (например, выбор витаминов, общие вопросы о цикле):**\n",
    "* **Хорошо:** <font color=\"#27AE60\">QLoRA</font>. Дает быстрый, конкретный ответ без длинных вступлений.\n",
    "* **Плохо:** <font color=\"#C0392B\">Base + RAG</font>. Генерирует избыточный текст на 500+ токенов там, где достаточно двух предложений.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=\"#1F618D\"><u>5. Стратегия улучшения результатов</u></font>\n",
    "\n",
    "Для достижения оптимального баланса рекомендуется:\n",
    "\n",
    "1. **Каскадная архитектура:** Использовать легкую модель для классификации запроса. Если запрос классифицирован как «высокорисковый», подключать тяжелую модель с RAG.\n",
    "2. **Жесткий пост-процессинг:** Внедрить отдельный блок проверки на «красные флаги», который будет принудительно добавляться к любому ответу, если в запросе есть ключевые слова-триггеры. (Outline, GuardRails)\n",
    "3. **Улучшение обучающих корпусов:** Для качественного QLoRA необходимо формировать вручную корпуса высококачественных данных QA.\n",
    "4. **Сжатие документов RAG:** Вместо подачи всего документа в промпт, использовать компрессор фрагментов для снижения Latency PISCO (Pretty Simple Compression).\n",
    "PISCO сжимает каждый документ в небольшое число learnable memory tokens (непрерывных эмбеддингов), которые затем используются генератором вместо исходного текста, чтобы резко сократить контекст без потери смысла."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909de72",
   "metadata": {},
   "source": [
    "# Подготвка тестового датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1183be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8993c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Doctor-HealthCare-100k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3728648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.sample(100)\n",
    "\n",
    "instruction_text = (\"You are a licensed medical doctor. Respond in a professional, neutral, and explanatory tone.\")\n",
    "\n",
    "test_data[\"instruction\"] = instruction_text\n",
    "\n",
    "test_data.to_csv('final_test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92a252d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   instruction  100 non-null    object\n",
      " 1   input        100 non-null    object\n",
      " 2   output       100 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a licensed medical doctor. Respond in a professional, neutral, and explanatory tone.</td>\n",
       "      <td>well im 31 weeks and 6 days Today(March 11,2010) and my due date is May 7,2010..but im just wondering who could be my baby s daddy the 1st boyfriend or the 2nd boyfriend.me and my first boyfriend been together for 5-6 months but we broke up in the middle of June 2009.we been together(Feb.22-Middle of June)but me and him did unprotected Sex more then 8 times.But after we broke up we didnt see each other for a while.but right after we broke up a month after i didnt have my period...then on August 9 i had unprotected sex with my 2nd boyfriend only 2 times but i already didnt have my period that time because i had my last period before my second boyfriend...plus if my baby s daddy was the 2nd boyfriend wouldnt i be due in the end of May or the Beginning Of June...Who could be the Baby s Daddy?</td>\n",
       "      <td>Hallow Dear, I would have appreciated had you mentioned the date of your last menstrual period. However, the very fact you had your last menstrual period before you had intercourse with your second boyfriend; i.e. after you have separated from the first boyfriend, the chances of first boyfriend being the father of this child are nil. It has to be due to the intercourse which has taken place after you had your last menstrual period. So naturally, your second boyfriend happens to be the responsible person for this pregnancy. However, if you are still in doubt, you may get DNA test done for confirming the paternity. I hope this resolves your confusion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are a licensed medical doctor. Respond in a professional, neutral, and explanatory tone.</td>\n",
       "      <td>Which is the best folic acid tablet to take while trying to conceive? And what other multivit tablets (names please) are good to take in addition to folinine(?) or folinz(?) I know of Folinine and Folinext. Not sure which to take. My doctor initially prescribed folinext, and later changed it to folinine. Now I am reading on your website about zinc in folinz. Please advise.</td>\n",
       "      <td>It is recommended that you get at least 400mcg of folate daily when trying to conceive.  There is not one brand of folate that is better than another.  Also remember, you can foliate from food that are fortified with it, such as orange juice.  You can google \\\"folate rich foods\\\" and get a nice list. Also, most generic over the counter \\\"prenatal vitamins\\\" contain 800mcg of folate.  This would be adequate for anyone trying to get pregnant.  From my experience, the only advantage of one pill over another is potentially the SIZE of the pill.  Often prenatal are large pills, but some of the more expensive name brands are smaller. I hope this helps!  Good luck!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are a licensed medical doctor. Respond in a professional, neutral, and explanatory tone.</td>\n",
       "      <td>Hi, my baby son is 14months old and he is having intermittently high fever. i gave him paracetemol this morning. now, in the evening he has shivers foolowed by fever. i gave hime paracetomol again. the fever is dying. should i be worried. should i take him to A&amp;E?</td>\n",
       "      <td>Hello, Fever of a few days without any localizing signs could as well a viral illness. Usually, rather than fever, what is more, important is the activity of the child, in between 2 fever episodes on the same day. If the kid is active and playing around when there is no fever, it is probably a viral illness and it doesn't require antibiotics at all. Once the viral fever comes it will there for 4-7 days. So do not worry about the duration if the kid is active. Paracetamol can be given that too only if fever is more than 100F. I suggest not using combination medicines for fever, especially with Paracetamol. Hope I have answered your query. Let me know if I can assist you further.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    instruction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             input                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          output\n",
       "0  You are a licensed medical doctor. Respond in a professional, neutral, and explanatory tone.  well im 31 weeks and 6 days Today(March 11,2010) and my due date is May 7,2010..but im just wondering who could be my baby s daddy the 1st boyfriend or the 2nd boyfriend.me and my first boyfriend been together for 5-6 months but we broke up in the middle of June 2009.we been together(Feb.22-Middle of June)but me and him did unprotected Sex more then 8 times.But after we broke up we didnt see each other for a while.but right after we broke up a month after i didnt have my period...then on August 9 i had unprotected sex with my 2nd boyfriend only 2 times but i already didnt have my period that time because i had my last period before my second boyfriend...plus if my baby s daddy was the 2nd boyfriend wouldnt i be due in the end of May or the Beginning Of June...Who could be the Baby s Daddy?                               Hallow Dear, I would have appreciated had you mentioned the date of your last menstrual period. However, the very fact you had your last menstrual period before you had intercourse with your second boyfriend; i.e. after you have separated from the first boyfriend, the chances of first boyfriend being the father of this child are nil. It has to be due to the intercourse which has taken place after you had your last menstrual period. So naturally, your second boyfriend happens to be the responsible person for this pregnancy. However, if you are still in doubt, you may get DNA test done for confirming the paternity. I hope this resolves your confusion.\n",
       "1  You are a licensed medical doctor. Respond in a professional, neutral, and explanatory tone.                                                                                                                                                                                                                                                                                                                                                                                                                                           Which is the best folic acid tablet to take while trying to conceive? And what other multivit tablets (names please) are good to take in addition to folinine(?) or folinz(?) I know of Folinine and Folinext. Not sure which to take. My doctor initially prescribed folinext, and later changed it to folinine. Now I am reading on your website about zinc in folinz. Please advise.                      It is recommended that you get at least 400mcg of folate daily when trying to conceive.  There is not one brand of folate that is better than another.  Also remember, you can foliate from food that are fortified with it, such as orange juice.  You can google \\\"folate rich foods\\\" and get a nice list. Also, most generic over the counter \\\"prenatal vitamins\\\" contain 800mcg of folate.  This would be adequate for anyone trying to get pregnant.  From my experience, the only advantage of one pill over another is potentially the SIZE of the pill.  Often prenatal are large pills, but some of the more expensive name brands are smaller. I hope this helps!  Good luck!\n",
       "2  You are a licensed medical doctor. Respond in a professional, neutral, and explanatory tone.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Hi, my baby son is 14months old and he is having intermittently high fever. i gave him paracetemol this morning. now, in the evening he has shivers foolowed by fever. i gave hime paracetomol again. the fever is dying. should i be worried. should i take him to A&E?  Hello, Fever of a few days without any localizing signs could as well a viral illness. Usually, rather than fever, what is more, important is the activity of the child, in between 2 fever episodes on the same day. If the kid is active and playing around when there is no fever, it is probably a viral illness and it doesn't require antibiotics at all. Once the viral fever comes it will there for 4-7 days. So do not worry about the duration if the kid is active. Paracetamol can be given that too only if fever is more than 100F. I suggest not using combination medicines for fever, especially with Paracetamol. Hope I have answered your query. Let me know if I can assist you further."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('final_test_data.csv')\n",
    "print(test_data.info())\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54006b1",
   "metadata": {},
   "source": [
    "# Создание json для дальнейшего анализа"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8264b853",
   "metadata": {},
   "source": [
    "**Данный json файл аккумулирует вопросы пациентов, ответы врачей, output из систем и оценки от LLM-as-a-Judge**\n",
    "\n",
    "**Дополнительно я создаю комбинации Prompt/RAG/LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e45662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл evaluation_sistems.json успешно создан. Количество записей: 100\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('final_test_data.csv')\n",
    "\n",
    "# 1. Список метрик для оценки (добавляем во все блоки)\n",
    "evaluation_metrics = [\n",
    "    \"Clinical correctness & factual accuracy\",\n",
    "    \"Appropriate triage & urgency guidance\",\n",
    "    \"Safety / harm minimization\",\n",
    "    \"Differential diagnosis quality (breadth + plausibility)\",\n",
    "    \"Actionability & clarity of plan\",\n",
    "    \"Red flags & symptom checking\",\n",
    "    \"Communication quality (empathy + tone + readability)\"\n",
    "]\n",
    "\n",
    "# 2. Список вариаций моделей (Model Zoo)\n",
    "model_variants = [\n",
    "    \"Base_model_Qwen3B\",\n",
    "    \"Base_model_Qwen3B_with_system_prompt\",\n",
    "    \"Base_model_Qwen3B_with_RAG\",\n",
    "    \"Base_model_Qwen3B_with_system_prompt_with_RAG\",\n",
    "    \"Cleaned_DATA_QLoRA\",\n",
    "    \"Cleaned_DATA_QLoRA_with_system_prompt\",\n",
    "    \"Cleaned_DATA_QLoRA_with_RAG\",\n",
    "    \"Cleaned_DATA_QLoRA_with_system_prompt_with_RAG\"\n",
    "]\n",
    "\n",
    "# 3. Базовая структура пустых технических полей для моделей\n",
    "empty_model_stats = {\n",
    "    \"output\": \"\",\n",
    "    \"latency_sec\": None,\n",
    "    \"prompt_tokens\": None,\n",
    "    \"generated_tokens\": None,\n",
    "    \"total_tokens\": None\n",
    "}\n",
    "\n",
    "final_json_structure = {}\n",
    "\n",
    "# 4. Итерация по первым 100 строкам\n",
    "for index, row in test_data.iterrows():\n",
    "    # Ключ записи (строка \"1\", \"2\" и т.д., начиная с 1, а не 0)\n",
    "    entry_id = str(index + 1)\n",
    "    \n",
    "    # Формируем блок текущей записи\n",
    "    entry_block = {\n",
    "        \"Query\": row['input'],\n",
    "        \"Doctor\": {\n",
    "            \"Doctor_answer\": row['output']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # --- БЛОК ДОКТОРА ---\n",
    "    # Добавляем метрики оценки для Доктора (значения пустые)\n",
    "    for metric in evaluation_metrics:\n",
    "        entry_block[\"Doctor\"][metric] = None\n",
    "\n",
    "    # --- БЛОКИ МОДЕЛЕЙ ---\n",
    "    for model_name in model_variants:\n",
    "        # Копируем структуру технических метрик\n",
    "        model_block = empty_model_stats.copy()\n",
    "        \n",
    "        # Добавляем метрики оценки качества (значения пустые)\n",
    "        for metric in evaluation_metrics:\n",
    "            model_block[metric] = None\n",
    "            \n",
    "        # Записываем блок модели в общую структуру записи\n",
    "        entry_block[model_name] = model_block\n",
    "\n",
    "    # Добавляем готовую запись в общий словарь\n",
    "    final_json_structure[entry_id] = entry_block\n",
    "\n",
    "# 5. Сохранение в файл\n",
    "output_filename = 'evaluation_sistems.json'\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_json_structure, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Файл {output_filename} успешно создан. Количество записей: {len(final_json_structure)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb5ae60",
   "metadata": {},
   "source": [
    "# BaseLine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308a035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results saved to ./evaluation_sistems.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "OUTPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "MODEL_PATH = \"./Base_Line_Qwen3B_MLX_INT8\"\n",
    "SISTEM = \"Base_model_Qwen3B\"\n",
    "SYSTEM_PROMPT = \"You are a licensed medical doctor. Respond in a professional, neutral, and explanatory tone.\"\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model, tokenizer = load(MODEL_PATH)\n",
    "\n",
    "with open(INPUT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# for key, item in data.items():\n",
    "#     query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "# Для тестирования:\n",
    "for key, item in list(data.items())[:50]:\n",
    "    query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "    if not query_text:\n",
    "        print(f\"[{key}] SKIPPED (empty query)\\n\")\n",
    "        continue\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query_text},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    stdout_buffer = io.StringIO()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with redirect_stdout(stdout_buffer):\n",
    "        response = generate(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            verbose=True # Нам нужен verbose для парсинга токенов\n",
    "        )\n",
    "\n",
    "    latency_sec = round(time.time() - start_time, 4)\n",
    "    stdout_text = stdout_buffer.getvalue()\n",
    "\n",
    "    # --- PARSE TOKENS ---\n",
    "    prompt_tokens = 0\n",
    "    generated_tokens = 0\n",
    "    \n",
    "    # Извлекаем числа из строк вида \"Prompt: 45 tokens\" и \"Generation: 120 tokens\"\n",
    "    p_match = re.search(r\"Prompt:\\s+(\\d+)\", stdout_text)\n",
    "    g_match = re.search(r\"Generation:\\s+(\\d+)\", stdout_text)\n",
    "    \n",
    "    if p_match: prompt_tokens = int(p_match.group(1))\n",
    "    if g_match: generated_tokens = int(g_match.group(1))\n",
    "    total_tokens = prompt_tokens + generated_tokens\n",
    "\n",
    "    # --- UPDATE (NOT OVERWRITE) ---\n",
    "    # Используем .update(), чтобы не удалить ключи для оценки (Clinical correctness и т.д.)\n",
    "    item[SISTEM].update({\n",
    "        \"output\": response.strip(),\n",
    "        \"latency_sec\": latency_sec,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"generated_tokens\": generated_tokens,\n",
    "        \"total_tokens\": total_tokens\n",
    "    })\n",
    "\n",
    "\n",
    "# --- FINAL SAVE ---\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done! Results saved to {OUTPUT_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd6cc0",
   "metadata": {},
   "source": [
    "# BaseLine Model + Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbb1e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results saved to ./evaluation_sistems.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "OUTPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "MODEL_PATH = \"./Base_Line_Qwen3B_MLX_INT8\"\n",
    "SISTEM = \"Base_model_Qwen3B_with_system_prompt\"\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI doctor. Your goal is to write answers that look very similar\n",
    "in style and format to the reference answers in this dataset.\n",
    "\n",
    "EXAMPLE OF STYLE:\n",
    "\"Hi, Thanks for the query. Usually posterior subaerial fibroid may not affect the chances of pregnancy.\n",
    "But it can grow during pregnancy and can lead to complications like red degeneration, difficulty in labor,\n",
    "increasing the chances of surgical procedures etc. So, better to get treated for the fibroid before planning\n",
    "for pregnancy. The exact time taken for the fibroid to grow cannot be told as that can differ from person to\n",
    "person and depends on hormonal levels. Treatment options of fibroid depends on size, symptoms etc. Consult\n",
    "local gynecologist once and take her opinion. You can go for hormonal pills for few months, that can decrease\n",
    "the size of the fibroid. If fibroid is not decreasing or increases in size, you may need surgical procedure.\n",
    "Mastectomy can be done either by laparoscopic method or by laparotomy. After the recovery with your doctors'\n",
    "advice you can plan for pregnancy. Take care.\"\n",
    "\n",
    "INSTRUCTIONS FOR YOUR ANSWER:\n",
    "- Follow the same tone: polite, explanatory, slightly repetitive, focused on reassurance and guidance.\n",
    "- Use similar sentence length and paragraph style (continuous prose, no bullet points).\n",
    "- Mention common phrases like \"depends on\", \"consult local doctor\", \"take care\" when appropriate.\n",
    "- Adapt the medical content to the new question, but keep the overall style very close to the example.\n",
    "- Do NOT write lists or headings; use only plain sentences.\n",
    "- Do NOT refer to this example explicitly in your answer.\n",
    "\"\"\"\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model, tokenizer = load(MODEL_PATH)\n",
    "\n",
    "with open(INPUT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# for key, item in data.items():\n",
    "#     query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "# Для тестирования:\n",
    "for key, item in list(data.items())[:50]:\n",
    "    query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "    if not query_text:\n",
    "        print(f\"[{key}] SKIPPED (empty query)\\n\")\n",
    "        continue\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query_text},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    stdout_buffer = io.StringIO()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with redirect_stdout(stdout_buffer):\n",
    "        response = generate(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            verbose=True # Нам нужен verbose для парсинга токенов\n",
    "        )\n",
    "\n",
    "    latency_sec = round(time.time() - start_time, 4)\n",
    "    stdout_text = stdout_buffer.getvalue()\n",
    "\n",
    "    # --- PARSE TOKENS ---\n",
    "    prompt_tokens = 0\n",
    "    generated_tokens = 0\n",
    "    \n",
    "    # Извлекаем числа из строк вида \"Prompt: 45 tokens\" и \"Generation: 120 tokens\"\n",
    "    p_match = re.search(r\"Prompt:\\s+(\\d+)\", stdout_text)\n",
    "    g_match = re.search(r\"Generation:\\s+(\\d+)\", stdout_text)\n",
    "    \n",
    "    if p_match: prompt_tokens = int(p_match.group(1))\n",
    "    if g_match: generated_tokens = int(g_match.group(1))\n",
    "    total_tokens = prompt_tokens + generated_tokens\n",
    "\n",
    "    # --- UPDATE (NOT OVERWRITE) ---\n",
    "    # Используем .update(), чтобы не удалить ключи для оценки (Clinical correctness и т.д.)\n",
    "    item[SISTEM].update({\n",
    "        \"output\": response.strip(),\n",
    "        \"latency_sec\": latency_sec,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"generated_tokens\": generated_tokens,\n",
    "        \"total_tokens\": total_tokens\n",
    "    })\n",
    "\n",
    "\n",
    "# --- FINAL SAVE ---\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done! Results saved to {OUTPUT_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd2a89",
   "metadata": {},
   "source": [
    "# BaseLine Model + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70eba4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели эмбеддингов из: /Users/ivannemcenko/Models/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/x6r7l4gd2gd0sy7wrqxrpjyr0000gn/T/ipykernel_84872/1566823521.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebe3461a60644d7a940fdfc19554578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: /Users/ivannemcenko/Models/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка векторной базы из: medical_vector_store...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# === КОНФИГУРАЦИЯ ===\n",
    "DB_DIR = \"medical_vector_store\"\n",
    "LOCAL_MODEL_DIR = \"/Users/ivannemcenko/Models/all-MiniLM-L6-v2\"\n",
    "TOP_K = 3\n",
    "\n",
    "# Один раз загружаю модель эмбеддинга и Векторную Базу Данных\n",
    "print(f\"Загрузка модели эмбеддингов из: {LOCAL_MODEL_DIR}...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=LOCAL_MODEL_DIR,\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "        show_progress=False\n",
    "    )\n",
    "\n",
    "print(f\"Загрузка векторной базы из: {DB_DIR}...\")\n",
    "try:\n",
    "        vectorstore = FAISS.load_local(\n",
    "            DB_DIR,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "except Exception as e:\n",
    "        print(f\"Ошибка загрузки базы: {e}\")\n",
    "\n",
    "\n",
    "def get_doctor_context(query_text):\n",
    "    # Поиск документов\n",
    "    results = vectorstore.similarity_search_with_relevance_scores(\n",
    "        query_text, \n",
    "        k=TOP_K\n",
    "    )\n",
    "\n",
    "    if not results:\n",
    "        print(\"Результаты не найдены.\")\n",
    "        return \"\"\n",
    "\n",
    "    # === ФОРМИРОВАНИЕ DOCTOR_DOC ===\n",
    "    # Мы создаем список, куда сложим все найденные ответы врачей\n",
    "    doc_entries = []\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        # 1. Извлекаем ответ врача из метаданных\n",
    "        # Используем .get() на случай, если ключа вдруг нет, чтобы код не упал\n",
    "        doctor_response = doc.metadata.get('response', 'No response available in metadata.')\n",
    "        \n",
    "        # 2. Форматируем отдельный блок для контекста\n",
    "        entry = f\"--- RETRIEVED MEDICAL CASE #{i} (Score: {score:.2f}) ---\\nDOCTOR RESPONSE:\\n{doctor_response}\"\n",
    "        doc_entries.append(entry)\n",
    "\n",
    "    # 4. Склеиваем всё в одну строку переменную DOCTOR_DOC\n",
    "    # Добавляем заголовок раздела и объединяем кейсы через отступы\n",
    "    header = \"\\n\\n### REFERENCE CASES FROM MEDICAL DATABASE:\\n\"\n",
    "    DOCTOR_DOC = header + \"\\n\\n\".join(doc_entries)\n",
    "\n",
    "    return DOCTOR_DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10214cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results saved to ./evaluation_sistems.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "OUTPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "MODEL_PATH = \"./Base_Line_Qwen3B_MLX_INT8\"\n",
    "SISTEM = \"Base_model_Qwen3B_with_RAG\"\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an experienced medical doctor providing telemedicine consultation.\n",
    "\n",
    "CRITICAL GUIDELINES:\n",
    "1. Base your response on the provided similar medical cases\n",
    "2. Be professional, empathetic, and evidence-based\n",
    "3. Provide clear, actionable advice\n",
    "4. Include warning signs for emergency situations\n",
    "5. Always recommend consulting with a healthcare provider\n",
    "6. Never diagnose definitively without examination\n",
    "\n",
    "RESPONSE STRUCTURE:\n",
    "- Acknowledge the patient's concern\n",
    "- Reference similar cases if relevant\n",
    "- Provide practical guidance\n",
    "- List red flags requiring immediate attention\n",
    "- Suggest next steps\n",
    "\n",
    "DISCLAIMER: This is informational only, not medical advice.\n",
    "\"\"\"\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model, tokenizer = load(MODEL_PATH)\n",
    "\n",
    "with open(INPUT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for key, item in data.items():\n",
    "    query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "# Для тестирования:\n",
    "# for key, item in list(data.items())[:1]:\n",
    "#     query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "    if not query_text:\n",
    "        print(f\"[{key}] SKIPPED (empty query)\\n\")\n",
    "        continue\n",
    "\n",
    "    # RAG - логика\n",
    "    DOCTOR_DOC = get_doctor_context(query_text)\n",
    "    RAG_content = SYSTEM_PROMPT + DOCTOR_DOC\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_content},\n",
    "        {\"role\": \"user\", \"content\": query_text},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    stdout_buffer = io.StringIO()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with redirect_stdout(stdout_buffer):\n",
    "        response = generate(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            verbose=True # Нам нужен verbose для парсинга токенов\n",
    "        )\n",
    "\n",
    "    latency_sec = round(time.time() - start_time, 4)\n",
    "    stdout_text = stdout_buffer.getvalue()\n",
    "\n",
    "    # --- PARSE TOKENS ---\n",
    "    prompt_tokens = 0\n",
    "    generated_tokens = 0\n",
    "    \n",
    "    # Извлекаем числа из строк вида \"Prompt: 45 tokens\" и \"Generation: 120 tokens\"\n",
    "    p_match = re.search(r\"Prompt:\\s+(\\d+)\", stdout_text)\n",
    "    g_match = re.search(r\"Generation:\\s+(\\d+)\", stdout_text)\n",
    "    \n",
    "    if p_match: prompt_tokens = int(p_match.group(1))\n",
    "    if g_match: generated_tokens = int(g_match.group(1))\n",
    "    total_tokens = prompt_tokens + generated_tokens\n",
    "\n",
    "    # --- UPDATE (NOT OVERWRITE) ---\n",
    "    # Используем .update(), чтобы не удалить ключи для оценки (Clinical correctness и т.д.)\n",
    "    item[SISTEM].update({\n",
    "        \"output\": response.strip(),\n",
    "        \"latency_sec\": latency_sec,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"generated_tokens\": generated_tokens,\n",
    "        \"total_tokens\": total_tokens\n",
    "    })\n",
    "\n",
    "\n",
    "# --- FINAL SAVE ---\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done! Results saved to {OUTPUT_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0f86fd",
   "metadata": {},
   "source": [
    "# BaseLine Model + RAG + Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15cd221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели эмбеддингов из: /Users/ivannemcenko/Models/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/x6r7l4gd2gd0sy7wrqxrpjyr0000gn/T/ipykernel_82226/1566823521.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34081b75cb445c6a5ed16b9911b664d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: /Users/ivannemcenko/Models/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка векторной базы из: medical_vector_store...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# === КОНФИГУРАЦИЯ ===\n",
    "DB_DIR = \"medical_vector_store\"\n",
    "LOCAL_MODEL_DIR = \"/Users/ivannemcenko/Models/all-MiniLM-L6-v2\"\n",
    "TOP_K = 3\n",
    "\n",
    "# Один раз загружаю модель эмбеддинга и Векторную Базу Данных\n",
    "print(f\"Загрузка модели эмбеддингов из: {LOCAL_MODEL_DIR}...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=LOCAL_MODEL_DIR,\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "        show_progress=False\n",
    "    )\n",
    "\n",
    "print(f\"Загрузка векторной базы из: {DB_DIR}...\")\n",
    "try:\n",
    "        vectorstore = FAISS.load_local(\n",
    "            DB_DIR,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "except Exception as e:\n",
    "        print(f\"Ошибка загрузки базы: {e}\")\n",
    "\n",
    "\n",
    "def get_doctor_context(query_text):\n",
    "    # Поиск документов\n",
    "    results = vectorstore.similarity_search_with_relevance_scores(\n",
    "        query_text, \n",
    "        k=TOP_K\n",
    "    )\n",
    "\n",
    "    if not results:\n",
    "        print(\"Результаты не найдены.\")\n",
    "        return \"\"\n",
    "\n",
    "    # === ФОРМИРОВАНИЕ DOCTOR_DOC ===\n",
    "    # Мы создаем список, куда сложим все найденные ответы врачей\n",
    "    doc_entries = []\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        # 1. Извлекаем ответ врача из метаданных\n",
    "        # Используем .get() на случай, если ключа вдруг нет, чтобы код не упал\n",
    "        doctor_response = doc.metadata.get('response', 'No response available in metadata.')\n",
    "        \n",
    "        # 2. Форматируем отдельный блок для контекста\n",
    "        entry = f\"--- RETRIEVED MEDICAL CASE #{i} (Score: {score:.2f}) ---\\nDOCTOR RESPONSE:\\n{doctor_response}\"\n",
    "        doc_entries.append(entry)\n",
    "\n",
    "    # 4. Склеиваем всё в одну строку переменную DOCTOR_DOC\n",
    "    # Добавляем заголовок раздела и объединяем кейсы через отступы\n",
    "    header = \"\\n\\n### REFERENCE CASES FROM MEDICAL DATABASE:\\n\"\n",
    "    DOCTOR_DOC = header + \"\\n\\n\".join(doc_entries)\n",
    "\n",
    "    return DOCTOR_DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576b80db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results saved to ./evaluation_sistems.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "OUTPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "MODEL_PATH = \"./Base_Line_Qwen3B_MLX_INT8\"\n",
    "SISTEM = \"Base_model_Qwen3B_with_system_prompt_with_RAG\"\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI doctor. Your goal is to write answers that look very similar\n",
    "in style and format to the reference answers in this dataset.\n",
    "\n",
    "EXAMPLE OF STYLE:\n",
    "\"Hi, Thanks for the query. Usually posterior subaerial fibroid may not affect the chances of pregnancy.\n",
    "But it can grow during pregnancy and can lead to complications like red degeneration, difficulty in labor,\n",
    "increasing the chances of surgical procedures etc. So, better to get treated for the fibroid before planning\n",
    "for pregnancy. The exact time taken for the fibroid to grow cannot be told as that can differ from person to\n",
    "person and depends on hormonal levels. Treatment options of fibroid depends on size, symptoms etc. Consult\n",
    "local gynecologist once and take her opinion. You can go for hormonal pills for few months, that can decrease\n",
    "the size of the fibroid. If fibroid is not decreasing or increases in size, you may need surgical procedure.\n",
    "Mastectomy can be done either by laparoscopic method or by laparotomy. After the recovery with your doctors'\n",
    "advice you can plan for pregnancy. Take care.\"\n",
    "\n",
    "INSTRUCTIONS FOR YOUR ANSWER:\n",
    "- Follow the same tone: polite, explanatory, slightly repetitive, focused on reassurance and guidance.\n",
    "- Use similar sentence length and paragraph style (continuous prose, no bullet points).\n",
    "- Mention common phrases like \"depends on\", \"consult local doctor\", \"take care\" when appropriate.\n",
    "- Adapt the medical content to the new question, but keep the overall style very close to the example.\n",
    "- Do NOT write lists or headings; use only plain sentences.\n",
    "- Do NOT refer to this example explicitly in your answer.\n",
    "\"\"\"\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model, tokenizer = load(MODEL_PATH)\n",
    "\n",
    "with open(INPUT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# for key, item in data.items():\n",
    "#     query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "# Для тестирования:\n",
    "for key, item in list(data.items())[:50]:\n",
    "    query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "    if not query_text:\n",
    "        print(f\"[{key}] SKIPPED (empty query)\\n\")\n",
    "        continue\n",
    "\n",
    "    # RAG - логика\n",
    "    DOCTOR_DOC = get_doctor_context(query_text)\n",
    "    RAG_content = SYSTEM_PROMPT + DOCTOR_DOC\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_content},\n",
    "        {\"role\": \"user\", \"content\": query_text},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    stdout_buffer = io.StringIO()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with redirect_stdout(stdout_buffer):\n",
    "        response = generate(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            verbose=True # Нам нужен verbose для парсинга токенов\n",
    "        )\n",
    "\n",
    "    latency_sec = round(time.time() - start_time, 4)\n",
    "    stdout_text = stdout_buffer.getvalue()\n",
    "\n",
    "    # --- PARSE TOKENS ---\n",
    "    prompt_tokens = 0\n",
    "    generated_tokens = 0\n",
    "    \n",
    "    # Извлекаем числа из строк вида \"Prompt: 45 tokens\" и \"Generation: 120 tokens\"\n",
    "    p_match = re.search(r\"Prompt:\\s+(\\d+)\", stdout_text)\n",
    "    g_match = re.search(r\"Generation:\\s+(\\d+)\", stdout_text)\n",
    "    \n",
    "    if p_match: prompt_tokens = int(p_match.group(1))\n",
    "    if g_match: generated_tokens = int(g_match.group(1))\n",
    "    total_tokens = prompt_tokens + generated_tokens\n",
    "\n",
    "    # --- UPDATE (NOT OVERWRITE) ---\n",
    "    # Используем .update(), чтобы не удалить ключи для оценки (Clinical correctness и т.д.)\n",
    "    item[SISTEM].update({\n",
    "        \"output\": response.strip(),\n",
    "        \"latency_sec\": latency_sec,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"generated_tokens\": generated_tokens,\n",
    "        \"total_tokens\": total_tokens\n",
    "    })\n",
    "\n",
    "\n",
    "# --- FINAL SAVE ---\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done! Results saved to {OUTPUT_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b0c30",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb2d98b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'Qwen3B_MLX_INT8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results saved to ./evaluation_sistems.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "OUTPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "MODEL_PATH = \"./Qwen3B_MLX_INT8\"\n",
    "SISTEM = \"Cleaned_DATA_QLoRA\"\n",
    "SYSTEM_PROMPT = \"You are a licensed medical doctor. Respond in a professional, neutral, and explanatory tone.\"\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model, tokenizer = load(MODEL_PATH)\n",
    "\n",
    "with open(INPUT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for key, item in data.items():\n",
    "    query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "# Для тестирования:\n",
    "# for key, item in list(data.items())[:1]:\n",
    "#     query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "    if not query_text:\n",
    "        print(f\"[{key}] SKIPPED (empty query)\\n\")\n",
    "        continue\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query_text},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    stdout_buffer = io.StringIO()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with redirect_stdout(stdout_buffer):\n",
    "        response = generate(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            verbose=True # Нам нужен verbose для парсинга токенов\n",
    "        )\n",
    "\n",
    "    latency_sec = round(time.time() - start_time, 4)\n",
    "    stdout_text = stdout_buffer.getvalue()\n",
    "\n",
    "    # --- PARSE TOKENS ---\n",
    "    prompt_tokens = 0\n",
    "    generated_tokens = 0\n",
    "    \n",
    "    # Извлекаем числа из строк вида \"Prompt: 45 tokens\" и \"Generation: 120 tokens\"\n",
    "    p_match = re.search(r\"Prompt:\\s+(\\d+)\", stdout_text)\n",
    "    g_match = re.search(r\"Generation:\\s+(\\d+)\", stdout_text)\n",
    "    \n",
    "    if p_match: prompt_tokens = int(p_match.group(1))\n",
    "    if g_match: generated_tokens = int(g_match.group(1))\n",
    "    total_tokens = prompt_tokens + generated_tokens\n",
    "\n",
    "    # --- UPDATE (NOT OVERWRITE) ---\n",
    "    # Используем .update(), чтобы не удалить ключи для оценки (Clinical correctness и т.д.)\n",
    "    item[SISTEM].update({\n",
    "        \"output\": response.strip(),\n",
    "        \"latency_sec\": latency_sec,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"generated_tokens\": generated_tokens,\n",
    "        \"total_tokens\": total_tokens\n",
    "    })\n",
    "\n",
    "\n",
    "# --- FINAL SAVE ---\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done! Results saved to {OUTPUT_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c9418",
   "metadata": {},
   "source": [
    "# LoRA + Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596297b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'Qwen3B_MLX_INT8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results saved to ./evaluation_sistems.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "OUTPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "MODEL_PATH = \"./Qwen3B_MLX_INT8\"\n",
    "SISTEM = \"Cleaned_DATA_QLoRA_with_system_prompt\"\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI doctor. Your goal is to write answers that look very similar\n",
    "in style and format to the reference answers in this dataset.\n",
    "\n",
    "EXAMPLE OF STYLE:\n",
    "\"Hi, Thanks for the query. Usually posterior subaerial fibroid may not affect the chances of pregnancy.\n",
    "But it can grow during pregnancy and can lead to complications like red degeneration, difficulty in labor,\n",
    "increasing the chances of surgical procedures etc. So, better to get treated for the fibroid before planning\n",
    "for pregnancy. The exact time taken for the fibroid to grow cannot be told as that can differ from person to\n",
    "person and depends on hormonal levels. Treatment options of fibroid depends on size, symptoms etc. Consult\n",
    "local gynecologist once and take her opinion. You can go for hormonal pills for few months, that can decrease\n",
    "the size of the fibroid. If fibroid is not decreasing or increases in size, you may need surgical procedure.\n",
    "Mastectomy can be done either by laparoscopic method or by laparotomy. After the recovery with your doctors'\n",
    "advice you can plan for pregnancy. Take care.\"\n",
    "\n",
    "INSTRUCTIONS FOR YOUR ANSWER:\n",
    "- Follow the same tone: polite, explanatory, slightly repetitive, focused on reassurance and guidance.\n",
    "- Use similar sentence length and paragraph style (continuous prose, no bullet points).\n",
    "- Mention common phrases like \"depends on\", \"consult local doctor\", \"take care\" when appropriate.\n",
    "- Adapt the medical content to the new question, but keep the overall style very close to the example.\n",
    "- Do NOT write lists or headings; use only plain sentences.\n",
    "- Do NOT refer to this example explicitly in your answer.\n",
    "\"\"\"\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model, tokenizer = load(MODEL_PATH)\n",
    "\n",
    "with open(INPUT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for key, item in data.items():\n",
    "    query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "# Для тестирования:\n",
    "# for key, item in list(data.items())[:1]:\n",
    "#     query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "    if not query_text:\n",
    "        print(f\"[{key}] SKIPPED (empty query)\\n\")\n",
    "        continue\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query_text},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    stdout_buffer = io.StringIO()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with redirect_stdout(stdout_buffer):\n",
    "        response = generate(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            verbose=True # Нам нужен verbose для парсинга токенов\n",
    "        )\n",
    "\n",
    "    latency_sec = round(time.time() - start_time, 4)\n",
    "    stdout_text = stdout_buffer.getvalue()\n",
    "\n",
    "    # --- PARSE TOKENS ---\n",
    "    prompt_tokens = 0\n",
    "    generated_tokens = 0\n",
    "    \n",
    "    # Извлекаем числа из строк вида \"Prompt: 45 tokens\" и \"Generation: 120 tokens\"\n",
    "    p_match = re.search(r\"Prompt:\\s+(\\d+)\", stdout_text)\n",
    "    g_match = re.search(r\"Generation:\\s+(\\d+)\", stdout_text)\n",
    "    \n",
    "    if p_match: prompt_tokens = int(p_match.group(1))\n",
    "    if g_match: generated_tokens = int(g_match.group(1))\n",
    "    total_tokens = prompt_tokens + generated_tokens\n",
    "\n",
    "    # --- UPDATE (NOT OVERWRITE) ---\n",
    "    # Используем .update(), чтобы не удалить ключи для оценки (Clinical correctness и т.д.)\n",
    "    item[SISTEM].update({\n",
    "        \"output\": response.strip(),\n",
    "        \"latency_sec\": latency_sec,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"generated_tokens\": generated_tokens,\n",
    "        \"total_tokens\": total_tokens\n",
    "    })\n",
    "\n",
    "\n",
    "# --- FINAL SAVE ---\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done! Results saved to {OUTPUT_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80dd4f",
   "metadata": {},
   "source": [
    "# LoRA + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe5ca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели эмбеддингов из: /Users/ivannemcenko/Models/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/x6r7l4gd2gd0sy7wrqxrpjyr0000gn/T/ipykernel_87783/3703796932.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232375c739d345b09043428531917da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: /Users/ivannemcenko/Models/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка векторной базы из: medical_vector_store...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# === КОНФИГУРАЦИЯ ===\n",
    "DB_DIR = \"medical_vector_store\"\n",
    "LOCAL_MODEL_DIR = \"/Users/ivannemcenko/Models/all-MiniLM-L6-v2\"\n",
    "TOP_K = 3\n",
    "\n",
    "# Один раз загружаю модель эмбеддинга и Векторную Базу Данных\n",
    "print(f\"Загрузка модели эмбеддингов из: {LOCAL_MODEL_DIR}...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=LOCAL_MODEL_DIR,\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "        show_progress=False\n",
    "    )\n",
    "\n",
    "print(f\"Загрузка векторной базы из: {DB_DIR}...\")\n",
    "try:\n",
    "        vectorstore = FAISS.load_local(\n",
    "            DB_DIR,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "except Exception as e:\n",
    "        print(f\"Ошибка загрузки базы: {e}\")\n",
    "\n",
    "\n",
    "def get_doctor_context(query_text):\n",
    "    # Поиск документов\n",
    "    results = vectorstore.similarity_search_with_relevance_scores(\n",
    "        query_text, \n",
    "        k=TOP_K\n",
    "    )\n",
    "\n",
    "    if not results:\n",
    "        print(\"Результаты не найдены.\")\n",
    "        return \"\"\n",
    "\n",
    "    # === ФОРМИРОВАНИЕ DOCTOR_DOC ===\n",
    "    # Мы создаем список, куда сложим все найденные ответы\n",
    "    doc_entries = []\n",
    "    \n",
    "    # === ИЗМЕНЕННЫЙ УЧАСТОК КОДА ===\n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        # В PDF-базе основной текст находится в атрибуте page_content\n",
    "        pdf_content = doc.page_content\n",
    "        \n",
    "        # Также можем вытащить номер страницы, если он есть (PyPDFLoader его создает)\n",
    "        page_num = doc.metadata.get('page', 'N/A')\n",
    "        \n",
    "        # Форматируем блок. Вместо \"DOCTOR RESPONSE\" пишем \"DICTIONARY ENTRY/CONTENT\"\n",
    "        entry = (f\"--- RETRIEVED FROM DICTIONARY #{i} (Page: {page_num}, Score: {score:.2f}) ---\\n\"\n",
    "                 f\"CONTENT:\\n{pdf_content}\")\n",
    "        \n",
    "        doc_entries.append(entry)\n",
    "    # === КОНЕЦ ИЗМЕНЕННОГО УЧАСТКА ===\n",
    "\n",
    "    # 4. Склеиваем всё в одну строку переменную DOCTOR_DOC\n",
    "    # Добавляем заголовок раздела и объединяем кейсы через отступы\n",
    "    header = \"\\n\\n### REFERENCE DATA FROM MEDICAL DICTIONARY:\\n\"\n",
    "    DOCTOR_DOC = header + \"\\n\\n\".join(doc_entries)\n",
    "\n",
    "    return DOCTOR_DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a3668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'Qwen3B_MLX_INT8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results saved to ./evaluation_sistems.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "OUTPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "MODEL_PATH = \"./Qwen3B_MLX_INT8\"\n",
    "SISTEM = \"Cleaned_DATA_QLoRA_with_RAG\"\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an experienced medical doctor providing telemedicine consultation.\n",
    "\n",
    "CRITICAL GUIDELINES:\n",
    "1. Base your response on the provided similar medical cases\n",
    "2. Be professional, empathetic, and evidence-based\n",
    "3. Provide clear, actionable advice\n",
    "4. Include warning signs for emergency situations\n",
    "5. Always recommend consulting with a healthcare provider\n",
    "6. Never diagnose definitively without examination\n",
    "\n",
    "RESPONSE STRUCTURE:\n",
    "- Acknowledge the patient's concern\n",
    "- Reference similar cases if relevant\n",
    "- Provide practical guidance\n",
    "- List red flags requiring immediate attention\n",
    "- Suggest next steps\n",
    "\n",
    "DISCLAIMER: This is informational only, not medical advice.\n",
    "\"\"\"\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model, tokenizer = load(MODEL_PATH)\n",
    "\n",
    "with open(INPUT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# for key, item in data.items():\n",
    "#     query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "# Для тестирования:\n",
    "for key, item in list(data.items())[:50]:\n",
    "    query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "    # if not query_text:\n",
    "    #     print(f\"[{key}] SKIPPED (empty query)\\n\")\n",
    "    #     continue\n",
    "\n",
    "    # RAG - логика\n",
    "    DOCTOR_DOC = get_doctor_context(query_text)\n",
    "    RAG_content = SYSTEM_PROMPT + DOCTOR_DOC\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_content},\n",
    "        {\"role\": \"user\", \"content\": query_text},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    stdout_buffer = io.StringIO()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with redirect_stdout(stdout_buffer):\n",
    "        response = generate(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            verbose=True # Нам нужен verbose для парсинга токенов\n",
    "        )\n",
    "\n",
    "    latency_sec = round(time.time() - start_time, 4)\n",
    "    stdout_text = stdout_buffer.getvalue()\n",
    "\n",
    "    # --- PARSE TOKENS ---\n",
    "    prompt_tokens = 0\n",
    "    generated_tokens = 0\n",
    "    \n",
    "    # Извлекаем числа из строк вида \"Prompt: 45 tokens\" и \"Generation: 120 tokens\"\n",
    "    p_match = re.search(r\"Prompt:\\s+(\\d+)\", stdout_text)\n",
    "    g_match = re.search(r\"Generation:\\s+(\\d+)\", stdout_text)\n",
    "    \n",
    "    if p_match: prompt_tokens = int(p_match.group(1))\n",
    "    if g_match: generated_tokens = int(g_match.group(1))\n",
    "    total_tokens = prompt_tokens + generated_tokens\n",
    "\n",
    "    # --- UPDATE (NOT OVERWRITE) ---\n",
    "    # Используем .update(), чтобы не удалить ключи для оценки (Clinical correctness и т.д.)\n",
    "    item[SISTEM].update({\n",
    "        \"output\": response.strip(),\n",
    "        \"latency_sec\": latency_sec,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"generated_tokens\": generated_tokens,\n",
    "        \"total_tokens\": total_tokens\n",
    "    })\n",
    "\n",
    "\n",
    "# --- FINAL SAVE ---\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done! Results saved to {OUTPUT_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818496ac",
   "metadata": {},
   "source": [
    "# LoRA + Prompt + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3920381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели эмбеддингов из: /Users/ivannemcenko/Models/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/x6r7l4gd2gd0sy7wrqxrpjyr0000gn/T/ipykernel_90042/1566823521.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2ec67e502a442e9ff94461fec8674c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: /Users/ivannemcenko/Models/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка векторной базы из: medical_vector_store...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# === КОНФИГУРАЦИЯ ===\n",
    "DB_DIR = \"medical_vector_store\"\n",
    "LOCAL_MODEL_DIR = \"/Users/ivannemcenko/Models/all-MiniLM-L6-v2\"\n",
    "TOP_K = 3\n",
    "\n",
    "# Один раз загружаю модель эмбеддинга и Векторную Базу Данных\n",
    "print(f\"Загрузка модели эмбеддингов из: {LOCAL_MODEL_DIR}...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=LOCAL_MODEL_DIR,\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "        show_progress=False\n",
    "    )\n",
    "\n",
    "print(f\"Загрузка векторной базы из: {DB_DIR}...\")\n",
    "try:\n",
    "        vectorstore = FAISS.load_local(\n",
    "            DB_DIR,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "except Exception as e:\n",
    "        print(f\"Ошибка загрузки базы: {e}\")\n",
    "\n",
    "\n",
    "def get_doctor_context(query_text):\n",
    "    # Поиск документов\n",
    "    results = vectorstore.similarity_search_with_relevance_scores(\n",
    "        query_text, \n",
    "        k=TOP_K\n",
    "    )\n",
    "\n",
    "    if not results:\n",
    "        print(\"Результаты не найдены.\")\n",
    "        return \"\"\n",
    "\n",
    "    # === ФОРМИРОВАНИЕ DOCTOR_DOC ===\n",
    "    # Мы создаем список, куда сложим все найденные ответы врачей\n",
    "    doc_entries = []\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        # 1. Извлекаем ответ врача из метаданных\n",
    "        # Используем .get() на случай, если ключа вдруг нет, чтобы код не упал\n",
    "        doctor_response = doc.metadata.get('response', 'No response available in metadata.')\n",
    "        \n",
    "        # 2. Форматируем отдельный блок для контекста\n",
    "        entry = f\"--- RETRIEVED MEDICAL CASE #{i} (Score: {score:.2f}) ---\\nDOCTOR RESPONSE:\\n{doctor_response}\"\n",
    "        doc_entries.append(entry)\n",
    "\n",
    "    # 4. Склеиваем всё в одну строку переменную DOCTOR_DOC\n",
    "    # Добавляем заголовок раздела и объединяем кейсы через отступы\n",
    "    header = \"\\n\\n### REFERENCE CASES FROM MEDICAL DATABASE:\\n\"\n",
    "    DOCTOR_DOC = header + \"\\n\\n\".join(doc_entries)\n",
    "\n",
    "    return DOCTOR_DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279adf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'Qwen3B_MLX_INT8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results saved to ./evaluation_sistems.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "from contextlib import redirect_stdout\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "OUTPUT_JSON_PATH = \"./evaluation_sistems.json\"\n",
    "MODEL_PATH = \"./Qwen3B_MLX_INT8\"\n",
    "SISTEM = \"Cleaned_DATA_QLoRA_with_system_prompt_with_RAG\"\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI doctor. Your goal is to write answers that look very similar\n",
    "in style and format to the reference answers in this dataset.\n",
    "\n",
    "EXAMPLE OF STYLE:\n",
    "\"Hi, Thanks for the query. Usually posterior subaerial fibroid may not affect the chances of pregnancy.\n",
    "But it can grow during pregnancy and can lead to complications like red degeneration, difficulty in labor,\n",
    "increasing the chances of surgical procedures etc. So, better to get treated for the fibroid before planning\n",
    "for pregnancy. The exact time taken for the fibroid to grow cannot be told as that can differ from person to\n",
    "person and depends on hormonal levels. Treatment options of fibroid depends on size, symptoms etc. Consult\n",
    "local gynecologist once and take her opinion. You can go for hormonal pills for few months, that can decrease\n",
    "the size of the fibroid. If fibroid is not decreasing or increases in size, you may need surgical procedure.\n",
    "Mastectomy can be done either by laparoscopic method or by laparotomy. After the recovery with your doctors'\n",
    "advice you can plan for pregnancy. Take care.\"\n",
    "\n",
    "INSTRUCTIONS FOR YOUR ANSWER:\n",
    "- Follow the same tone: polite, explanatory, slightly repetitive, focused on reassurance and guidance.\n",
    "- Use similar sentence length and paragraph style (continuous prose, no bullet points).\n",
    "- Mention common phrases like \"depends on\", \"consult local doctor\", \"take care\" when appropriate.\n",
    "- Adapt the medical content to the new question, but keep the overall style very close to the example.\n",
    "- Do NOT write lists or headings; use only plain sentences.\n",
    "- Do NOT refer to this example explicitly in your answer.\n",
    "\"\"\"\n",
    "MAX_TOKENS = 1024\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model, tokenizer = load(MODEL_PATH)\n",
    "\n",
    "with open(INPUT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for key, item in data.items():\n",
    "    query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "# # Для тестирования:\n",
    "# for key, item in list(data.items())[:1]:\n",
    "#     query_text = item.get(\"Query\", \"\").strip()\n",
    "\n",
    "    if not query_text:\n",
    "        print(f\"[{key}] SKIPPED (empty query)\\n\")\n",
    "        continue\n",
    "\n",
    "    # RAG - логика\n",
    "    DOCTOR_DOC = get_doctor_context(query_text)\n",
    "    RAG_content = SYSTEM_PROMPT + DOCTOR_DOC\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_content},\n",
    "        {\"role\": \"user\", \"content\": query_text},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    stdout_buffer = io.StringIO()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with redirect_stdout(stdout_buffer):\n",
    "        response = generate(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            verbose=True # Нам нужен verbose для парсинга токенов\n",
    "        )\n",
    "\n",
    "    latency_sec = round(time.time() - start_time, 4)\n",
    "    stdout_text = stdout_buffer.getvalue()\n",
    "\n",
    "    # --- PARSE TOKENS ---\n",
    "    prompt_tokens = 0\n",
    "    generated_tokens = 0\n",
    "    \n",
    "    # Извлекаем числа из строк вида \"Prompt: 45 tokens\" и \"Generation: 120 tokens\"\n",
    "    p_match = re.search(r\"Prompt:\\s+(\\d+)\", stdout_text)\n",
    "    g_match = re.search(r\"Generation:\\s+(\\d+)\", stdout_text)\n",
    "    \n",
    "    if p_match: prompt_tokens = int(p_match.group(1))\n",
    "    if g_match: generated_tokens = int(g_match.group(1))\n",
    "    total_tokens = prompt_tokens + generated_tokens\n",
    "\n",
    "    # --- UPDATE (NOT OVERWRITE) ---\n",
    "    # Используем .update(), чтобы не удалить ключи для оценки (Clinical correctness и т.д.)\n",
    "    item[SISTEM].update({\n",
    "        \"output\": response.strip(),\n",
    "        \"latency_sec\": latency_sec,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"generated_tokens\": generated_tokens,\n",
    "        \"total_tokens\": total_tokens\n",
    "    })\n",
    "\n",
    "\n",
    "# --- FINAL SAVE ---\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done! Results saved to {OUTPUT_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d3c67a",
   "metadata": {},
   "source": [
    "# Анализ оценок от LLM-as-a-Judge (Chat_GPT 5.2 и Gemini 3 Thinking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054b609",
   "metadata": {},
   "source": [
    "**Формирую таблицы для вывода метрик от судей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc32ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Успешно обработано файлов: 2\n",
      "\n",
      "Metric                                                  | Model                                              | Mean ± 95% CI        | N    \n",
      "============================================================================================================================================\n",
      "latency_sec                                             | Base_model_Qwen3B                                  | 7.79 ± 0.44          | 84   \n",
      "latency_sec                                             | Base_model_Qwen3B_with_system_prompt               | 8.01 ± 0.32          | 84   \n",
      "latency_sec                                             | Base_model_Qwen3B_with_RAG                         | 13.54 ± 0.56         | 84   \n",
      "latency_sec                                             | Base_model_Qwen3B_with_system_prompt_with_RAG      | 10.13 ± 0.45         | 84   \n",
      "latency_sec                                             | Cleaned_DATA_QLoRA                                 | 2.38 ± 0.12          | 84   \n",
      "latency_sec                                             | Cleaned_DATA_QLoRA_with_system_prompt              | 4.44 ± 0.70          | 84   \n",
      "latency_sec                                             | Cleaned_DATA_QLoRA_with_RAG                        | 6.28 ± 0.71          | 84   \n",
      "latency_sec                                             | Cleaned_DATA_QLoRA_with_system_prompt_with_RAG     | 4.96 ± 0.17          | 84   \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "prompt_tokens                                           | Base_model_Qwen3B                                  | 129.32 ± 8.18        | 84   \n",
      "prompt_tokens                                           | Base_model_Qwen3B_with_system_prompt               | 448.32 ± 8.18        | 84   \n",
      "prompt_tokens                                           | Base_model_Qwen3B_with_RAG                         | 714.90 ± 19.15       | 84   \n",
      "prompt_tokens                                           | Base_model_Qwen3B_with_system_prompt_with_RAG      | 922.90 ± 19.15       | 84   \n",
      "prompt_tokens                                           | Cleaned_DATA_QLoRA                                 | 129.32 ± 8.18        | 84   \n",
      "prompt_tokens                                           | Cleaned_DATA_QLoRA_with_system_prompt              | 448.32 ± 8.18        | 84   \n",
      "prompt_tokens                                           | Cleaned_DATA_QLoRA_with_RAG                        | 714.90 ± 19.15       | 84   \n",
      "prompt_tokens                                           | Cleaned_DATA_QLoRA_with_system_prompt_with_RAG     | 922.90 ± 19.15       | 84   \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "generated_tokens                                        | Base_model_Qwen3B                                  | 327.18 ± 19.41       | 84   \n",
      "generated_tokens                                        | Base_model_Qwen3B_with_system_prompt               | 295.23 ± 13.62       | 84   \n",
      "generated_tokens                                        | Base_model_Qwen3B_with_RAG                         | 500.48 ± 22.49       | 84   \n",
      "generated_tokens                                        | Base_model_Qwen3B_with_system_prompt_with_RAG      | 297.11 ± 16.07       | 84   \n",
      "generated_tokens                                        | Cleaned_DATA_QLoRA                                 | 85.54 ± 4.07         | 84   \n",
      "generated_tokens                                        | Cleaned_DATA_QLoRA_with_system_prompt              | 154.89 ± 30.56       | 84   \n",
      "generated_tokens                                        | Cleaned_DATA_QLoRA_with_RAG                        | 204.45 ± 29.78       | 84   \n",
      "generated_tokens                                        | Cleaned_DATA_QLoRA_with_system_prompt_with_RAG     | 134.14 ± 6.79        | 84   \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "total_tokens                                            | Base_model_Qwen3B                                  | 456.50 ± 23.82       | 84   \n",
      "total_tokens                                            | Base_model_Qwen3B_with_system_prompt               | 743.55 ± 18.44       | 84   \n",
      "total_tokens                                            | Base_model_Qwen3B_with_RAG                         | 1215.38 ± 36.51      | 84   \n",
      "total_tokens                                            | Base_model_Qwen3B_with_system_prompt_with_RAG      | 1220.01 ± 27.79      | 84   \n",
      "total_tokens                                            | Cleaned_DATA_QLoRA                                 | 214.86 ± 9.15        | 84   \n",
      "total_tokens                                            | Cleaned_DATA_QLoRA_with_system_prompt              | 603.21 ± 31.17       | 84   \n",
      "total_tokens                                            | Cleaned_DATA_QLoRA_with_RAG                        | 919.36 ± 33.65       | 84   \n",
      "total_tokens                                            | Cleaned_DATA_QLoRA_with_system_prompt_with_RAG     | 1057.05 ± 20.48      | 84   \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Clinical correctness & factual accuracy                 | Doctor_answer                                      | 3.41 ± 0.22          | 92   \n",
      "Clinical correctness & factual accuracy                 | Base_model_Qwen3B                                  | 4.01 ± 0.17          | 92   \n",
      "Clinical correctness & factual accuracy                 | Base_model_Qwen3B_with_system_prompt               | 3.85 ± 0.20          | 92   \n",
      "Clinical correctness & factual accuracy                 | Base_model_Qwen3B_with_RAG                         | 4.07 ± 0.19          | 92   \n",
      "Clinical correctness & factual accuracy                 | Base_model_Qwen3B_with_system_prompt_with_RAG      | 3.80 ± 0.20          | 92   \n",
      "Clinical correctness & factual accuracy                 | Cleaned_DATA_QLoRA                                 | 3.52 ± 0.20          | 92   \n",
      "Clinical correctness & factual accuracy                 | Cleaned_DATA_QLoRA_with_system_prompt              | 3.70 ± 0.18          | 92   \n",
      "Clinical correctness & factual accuracy                 | Cleaned_DATA_QLoRA_with_RAG                        | 3.80 ± 0.18          | 92   \n",
      "Clinical correctness & factual accuracy                 | Cleaned_DATA_QLoRA_with_system_prompt_with_RAG     | 3.18 ± 0.23          | 92   \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Appropriate triage & urgency guidance                   | Doctor_answer                                      | 3.24 ± 0.25          | 92   \n",
      "Appropriate triage & urgency guidance                   | Base_model_Qwen3B                                  | 4.17 ± 0.10          | 92   \n",
      "Appropriate triage & urgency guidance                   | Base_model_Qwen3B_with_system_prompt               | 3.91 ± 0.13          | 92   \n",
      "Appropriate triage & urgency guidance                   | Base_model_Qwen3B_with_RAG                         | 4.50 ± 0.12          | 92   \n",
      "Appropriate triage & urgency guidance                   | Base_model_Qwen3B_with_system_prompt_with_RAG      | 4.00 ± 0.13          | 92   \n",
      "Appropriate triage & urgency guidance                   | Cleaned_DATA_QLoRA                                 | 3.23 ± 0.16          | 92   \n",
      "Appropriate triage & urgency guidance                   | Cleaned_DATA_QLoRA_with_system_prompt              | 3.55 ± 0.17          | 92   \n",
      "Appropriate triage & urgency guidance                   | Cleaned_DATA_QLoRA_with_RAG                        | 3.90 ± 0.15          | 92   \n",
      "Appropriate triage & urgency guidance                   | Cleaned_DATA_QLoRA_with_system_prompt_with_RAG     | 3.25 ± 0.21          | 92   \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Safety / harm minimization                              | Doctor_answer                                      | 3.47 ± 0.25          | 92   \n",
      "Safety / harm minimization                              | Base_model_Qwen3B                                  | 4.37 ± 0.13          | 92   \n",
      "Safety / harm minimization                              | Base_model_Qwen3B_with_system_prompt               | 4.15 ± 0.16          | 92   \n",
      "Safety / harm minimization                              | Base_model_Qwen3B_with_RAG                         | 4.54 ± 0.14          | 92   \n",
      "Safety / harm minimization                              | Base_model_Qwen3B_with_system_prompt_with_RAG      | 4.16 ± 0.17          | 92   \n",
      "Safety / harm minimization                              | Cleaned_DATA_QLoRA                                 | 3.80 ± 0.14          | 92   \n",
      "Safety / harm minimization                              | Cleaned_DATA_QLoRA_with_system_prompt              | 3.97 ± 0.17          | 92   \n",
      "Safety / harm minimization                              | Cleaned_DATA_QLoRA_with_RAG                        | 4.16 ± 0.15          | 92   \n",
      "Safety / harm minimization                              | Cleaned_DATA_QLoRA_with_system_prompt_with_RAG     | 3.54 ± 0.23          | 92   \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Differential diagnosis quality (breadth + plausibility) | Doctor_answer                                      | 2.64 ± 0.23          | 92   \n",
      "Differential diagnosis quality (breadth + plausibility) | Base_model_Qwen3B                                  | 3.47 ± 0.19          | 92   \n",
      "Differential diagnosis quality (breadth + plausibility) | Base_model_Qwen3B_with_system_prompt               | 3.13 ± 0.18          | 92   \n",
      "Differential diagnosis quality (breadth + plausibility) | Base_model_Qwen3B_with_RAG                         | 3.65 ± 0.19          | 92   \n",
      "Differential diagnosis quality (breadth + plausibility) | Base_model_Qwen3B_with_system_prompt_with_RAG      | 3.15 ± 0.19          | 92   \n",
      "Differential diagnosis quality (breadth + plausibility) | Cleaned_DATA_QLoRA                                 | 2.53 ± 0.17          | 92   \n",
      "Differential diagnosis quality (breadth + plausibility) | Cleaned_DATA_QLoRA_with_system_prompt              | 2.87 ± 0.18          | 92   \n",
      "Differential diagnosis quality (breadth + plausibility) | Cleaned_DATA_QLoRA_with_RAG                        | 3.13 ± 0.19          | 92   \n",
      "Differential diagnosis quality (breadth + plausibility) | Cleaned_DATA_QLoRA_with_system_prompt_with_RAG     | 2.64 ± 0.20          | 92   \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Actionability & clarity of plan                         | Doctor_answer                                      | 3.22 ± 0.22          | 92   \n",
      "Actionability & clarity of plan                         | Base_model_Qwen3B                                  | 3.99 ± 0.12          | 92   \n",
      "Actionability & clarity of plan                         | Base_model_Qwen3B_with_system_prompt               | 3.55 ± 0.12          | 92   \n",
      "Actionability & clarity of plan                         | Base_model_Qwen3B_with_RAG                         | 4.73 ± 0.10          | 92   \n",
      "Actionability & clarity of plan                         | Base_model_Qwen3B_with_system_prompt_with_RAG      | 3.76 ± 0.14          | 92   \n",
      "Actionability & clarity of plan                         | Cleaned_DATA_QLoRA                                 | 2.90 ± 0.14          | 92   \n",
      "Actionability & clarity of plan                         | Cleaned_DATA_QLoRA_with_system_prompt              | 3.33 ± 0.15          | 92   \n",
      "Actionability & clarity of plan                         | Cleaned_DATA_QLoRA_with_RAG                        | 3.87 ± 0.15          | 92   \n",
      "Actionability & clarity of plan                         | Cleaned_DATA_QLoRA_with_system_prompt_with_RAG     | 3.14 ± 0.21          | 92   \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Red flags & symptom checking                            | Doctor_answer                                      | 1.68 ± 0.21          | 92   \n",
      "Red flags & symptom checking                            | Base_model_Qwen3B                                  | 2.36 ± 0.22          | 92   \n",
      "Red flags & symptom checking                            | Base_model_Qwen3B_with_system_prompt               | 2.08 ± 0.20          | 92   \n",
      "Red flags & symptom checking                            | Base_model_Qwen3B_with_RAG                         | 4.51 ± 0.14          | 92   \n",
      "Red flags & symptom checking                            | Base_model_Qwen3B_with_system_prompt_with_RAG      | 2.23 ± 0.21          | 92   \n",
      "Red flags & symptom checking                            | Cleaned_DATA_QLoRA                                 | 1.26 ± 0.12          | 92   \n",
      "Red flags & symptom checking                            | Cleaned_DATA_QLoRA_with_system_prompt              | 1.28 ± 0.13          | 92   \n",
      "Red flags & symptom checking                            | Cleaned_DATA_QLoRA_with_RAG                        | 2.20 ± 0.24          | 92   \n",
      "Red flags & symptom checking                            | Cleaned_DATA_QLoRA_with_system_prompt_with_RAG     | 1.32 ± 0.13          | 92   \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Communication quality (empathy + tone + readability)    | Doctor_answer                                      | 3.30 ± 0.19          | 92   \n",
      "Communication quality (empathy + tone + readability)    | Base_model_Qwen3B                                  | 4.54 ± 0.10          | 92   \n",
      "Communication quality (empathy + tone + readability)    | Base_model_Qwen3B_with_system_prompt               | 4.53 ± 0.13          | 92   \n",
      "Communication quality (empathy + tone + readability)    | Base_model_Qwen3B_with_RAG                         | 4.65 ± 0.12          | 92   \n",
      "Communication quality (empathy + tone + readability)    | Base_model_Qwen3B_with_system_prompt_with_RAG      | 4.43 ± 0.12          | 92   \n",
      "Communication quality (empathy + tone + readability)    | Cleaned_DATA_QLoRA                                 | 3.43 ± 0.13          | 92   \n",
      "Communication quality (empathy + tone + readability)    | Cleaned_DATA_QLoRA_with_system_prompt              | 3.96 ± 0.12          | 92   \n",
      "Communication quality (empathy + tone + readability)    | Cleaned_DATA_QLoRA_with_RAG                        | 4.11 ± 0.13          | 92   \n",
      "Communication quality (empathy + tone + readability)    | Cleaned_DATA_QLoRA_with_system_prompt_with_RAG     | 3.66 ± 0.17          | 92   \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Сгруппированный отчет по 2 файлам сохранен в: V2combined_metrics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "# 1. Анализ двух оценок от LLM-as-a-Judge (Chat_GPT 5.2 и Gemini 3 Thinking)\n",
    "\n",
    "INPUT_FILES = [\n",
    "    'Judge_GPT.json', \n",
    "    'Judge_Gemini.json'\n",
    "]\n",
    "\n",
    "TARGET_MODELS = {\n",
    "    \"Doctor_answer\": \"Doctor\",\n",
    "    \"Base_model_Qwen3B\": \"Base_model_Qwen3B\",\n",
    "    \"Base_model_Qwen3B_with_system_prompt\": \"Base_model_Qwen3B_with_system_prompt\",\n",
    "    \"Base_model_Qwen3B_with_RAG\": \"Base_model_Qwen3B_with_RAG\",\n",
    "    \"Base_model_Qwen3B_with_system_prompt_with_RAG\": \"Base_model_Qwen3B_with_system_prompt_with_RAG\",\n",
    "    \"Cleaned_DATA_QLoRA\": \"Cleaned_DATA_QLoRA\",\n",
    "    \"Cleaned_DATA_QLoRA_with_system_prompt\": \"Cleaned_DATA_QLoRA_with_system_prompt\",\n",
    "    \"Cleaned_DATA_QLoRA_with_RAG\": \"Cleaned_DATA_QLoRA_with_RAG\",\n",
    "    \"Cleaned_DATA_QLoRA_with_system_prompt_with_RAG\": \"Cleaned_DATA_QLoRA_with_system_prompt_with_RAG\"\n",
    "}\n",
    "\n",
    "METRICS = [\n",
    "    \"latency_sec\",\n",
    "    \"prompt_tokens\",\n",
    "    \"generated_tokens\",\n",
    "    \"total_tokens\",\n",
    "    \"Clinical correctness & factual accuracy\",\n",
    "    \"Appropriate triage & urgency guidance\",\n",
    "    \"Safety / harm minimization\",\n",
    "    \"Differential diagnosis quality (breadth + plausibility)\",\n",
    "    \"Actionability & clarity of plan\",\n",
    "    \"Red flags & symptom checking\",\n",
    "    \"Communication quality (empathy + tone + readability)\"\n",
    "]\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Считает среднее и размах доверительного интервала.\"\"\"\n",
    "    a = np.array(data, dtype=float)\n",
    "    n = len(a)\n",
    "    if n < 2:\n",
    "        return np.mean(a) if n == 1 else 0, 0\n",
    "    \n",
    "    m = np.mean(a)\n",
    "    se = stats.sem(a)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h\n",
    "\n",
    "def main():\n",
    "    # Единое хранилище для всех файлов\n",
    "    storage = {model: {metric: [] for metric in METRICS} for model in TARGET_MODELS.keys()}\n",
    "    \n",
    "    processed_files_count = 0\n",
    "\n",
    "    # 2. СБОР ДАННЫХ ИЗ НЕСКОЛЬКИХ ФАЙЛОВ\n",
    "    for file_path in INPUT_FILES:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Предупреждение: Файл {file_path} не найден и будет пропущен.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                processed_files_count += 1\n",
    "                \n",
    "                # Парсинг текущего файла\n",
    "                for item_id, content in data.items():\n",
    "                    for display_name, json_key in TARGET_MODELS.items():\n",
    "                        if json_key in content:\n",
    "                            block_data = content[json_key]\n",
    "                            for metric in METRICS:\n",
    "                                if metric in block_data:\n",
    "                                    val = block_data[metric]\n",
    "                                    if isinstance(val, (int, float)):\n",
    "                                        # Добавляем в общий котел\n",
    "                                        storage[display_name][metric].append(val)\n",
    "                                        \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Ошибка: Не удалось прочитать JSON из файла {file_path}.\")\n",
    "\n",
    "    if processed_files_count == 0:\n",
    "        print(\"Не обработано ни одного файла. Завершение.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Успешно обработано файлов: {processed_files_count}\")\n",
    "\n",
    "    # 3. ВЫВОД (СГРУППИРОВАНО ПО МЕТРИКАМ)\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\n{'Metric':<55} | {'Model':<50} | {'Mean ± 95% CI':<20} | {'N':<5}\")\n",
    "    print(\"=\" * 140)\n",
    "\n",
    "    for metric in METRICS:\n",
    "        has_data_for_metric = False\n",
    "        \n",
    "        for model in TARGET_MODELS.keys():\n",
    "            values = storage[model][metric]\n",
    "            \n",
    "            if not values:\n",
    "                continue \n",
    "\n",
    "            has_data_for_metric = True\n",
    "            mean_val, ci_val = calculate_confidence_interval(values)\n",
    "            count = len(values)\n",
    "            \n",
    "            res_string = f\"{mean_val:.2f} ± {ci_val:.2f}\"\n",
    "            \n",
    "            print(f\"{metric:<55} | {model:<50} | {res_string:<20} | {count:<5}\")\n",
    "            \n",
    "            results.append({\n",
    "                \"Metric\": metric,\n",
    "                \"Model\": model,\n",
    "                \"Mean\": mean_val,\n",
    "                \"CI_95\": ci_val,\n",
    "                \"Formatted\": res_string,\n",
    "                \"Count\": count\n",
    "            })\n",
    "        \n",
    "        if has_data_for_metric:\n",
    "            print(\"-\" * 140)\n",
    "\n",
    "    # 4. СОХРАНЕНИЕ\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        df = df[[\"Metric\", \"Model\", \"Mean\", \"CI_95\", \"Formatted\", \"Count\"]]\n",
    "        \n",
    "        output_csv = \"V2combined_metrics_summary.csv\"\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"\\nСгруппированный отчет по {processed_files_count} файлам сохранен в: {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MEDvenv)",
   "language": "python",
   "name": "medvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
